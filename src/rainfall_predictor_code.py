# -*- coding: utf-8 -*-
"""Rainfall-Predictor.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tc9WgZ2FZ74B28l82NMYgO0T8KnUVi0e

# Building a Rainfall Prediction Classifier

### Project Description

This notebook explores the use of machine learning to predict whether it will rain tomorrow based on historical weather data in Australia. The project includes data preprocessing, feature engineering, model training, and evaluation.

### Objectives

- Load and explore the Australian weather dataset
- Preprocess and clean the data
- Train and evaluate machine learning models
- Predict rain occurrence for the next day

### Tech Stack

- Jupyter Notebook

- **pandas**: For data loading and manipulation
- **matplotlib**, **seaborn**: For data visualization and plotting
- **scikit-learn**:
  - `ColumnTransformer`, `Pipeline`: For clean preprocessing pipelines
  - `StandardScaler`, `OneHotEncoder`: For feature scaling and encoding
  - `train_test_split`, `StratifiedKFold`, `GridSearchCV`: For model training and cross-validation
  - `RandomForestClassifier`, `LogisticRegression`: For classification modeling
  - `classification_report`, `confusion_matrix`, `ConfusionMatrixDisplay`: For performance evaluation

# About The Dataset
The original source of the data is Australian Government's Bureau of Meteorology and the latest data can be gathered from [http://www.bom.gov.au/climate/dwo/](http://www.bom.gov.au/climate/dwo/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML0101ENSkillsNetwork20718538-2022-01-01).

The dataset used in this project was downloaded from Kaggle at [https://www.kaggle.com/datasets/jsphyg/weather-dataset-rattle-package/](https://www.kaggle.com/datasets/jsphyg/weather-dataset-rattle-package?resource=download&select=weatherAUS.csv)  
Column definitions were gathered from [http://www.bom.gov.au/climate/dwo/IDCJDW0000.shtml](http://www.bom.gov.au/climate/dwo/IDCJDW0000.shtml?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML0101ENSkillsNetwork20718538-2022-01-01)  

The dataset contains observations of weather metrics for each day from 2008 to 2017, and includes the following fields:

| Field         | Description                                           | Unit            | Type   |
| :------------ | :---------------------------------------------------- | :-------------- | :----- |
| Date          | Date of the Observation in YYYY-MM-DD                 | Date            | object |
| Location      | Location of the Observation                           | Location        | object |
| MinTemp       | Minimum temperature                                   | Celsius         | float  |
| MaxTemp       | Maximum temperature                                   | Celsius         | float  |
| Rainfall      | Amount of rainfall                                    | Millimeters     | float  |
| Evaporation   | Amount of evaporation                                 | Millimeters     | float  |
| Sunshine      | Amount of bright sunshine                             | hours           | float  |
| WindGustDir   | Direction of the strongest gust                       | Compass Points  | object |
| WindGustSpeed | Speed of the strongest gust                           | Kilometers/Hour | object |
| WindDir9am    | Wind direction averaged over 10 minutes prior to 9am  | Compass Points  | object |
| WindDir3pm    | Wind direction averaged over 10 minutes prior to 3pm  | Compass Points  | object |
| WindSpeed9am  | Wind speed averaged over 10 minutes prior to 9am      | Kilometers/Hour | float  |
| WindSpeed3pm  | Wind speed averaged over 10 minutes prior to 3pm      | Kilometers/Hour | float  |
| Humidity9am   | Humidity at 9am                                       | Percent         | float  |
| Humidity3pm   | Humidity at 3pm                                       | Percent         | float  |
| Pressure9am   | Atmospheric pressure reduced to mean sea level at 9am | Hectopascal     | float  |
| Pressure3pm   | Atmospheric pressure reduced to mean sea level at 3pm | Hectopascal     | float  |
| Cloud9am      | Fraction of the sky obscured by cloud at 9am          | Eights          | float  |
| Cloud3pm      | Fraction of the sky obscured by cloud at 3pm          | Eights          | float  |
| Temp9am       | Temperature at 9am                                    | Celsius         | float  |
| Temp3pm       | Temperature at 3pm                                    | Celsius         | float  |
| RainToday     | If there was at least 1mm of rain today               | Yes/No          | object |
| RainTomorrow  | If there is at least 1mm of rain tomorrow             | Yes/No          | object |

## Install and import the required libraries

Exectue the following cell to download and import the necessary libraries.
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install pandas matplotlib scikit-learn seaborn

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import seaborn as sns
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score

"""## Load the data

Execute the following cells to load the dataset as a pandas dataframe.
"""

url="data/rainfall_data.csv"
df = pd.read_csv(url)
df.head()

df.count()

"""Sunshine and cloud cover seem like important features, but they have a lot of missing values, far too many to impute their missing values.

### Drop all rows with missing values
To try to keep things simple we'll drop rows with missing values and see what's left
"""

df = df.dropna()
df.info()

"""Since we still have 56k observations left after dropping missing values, we may not need to impute any missing values.  
Let's see how we do.

"""

df.columns

"""If we adjust our approach and aim to predict today’s rainfall using historical weather data up to and including yesterday, then we can legitimately utilize all of the available features. This shift would be particularly useful for practical applications, such as deciding whether you will bike to work today.

With this new target, we should update the names of the rain columns accordingly to avoid confusion.

"""

df = df.rename(columns={'RainToday': 'RainYesterday',
                        'RainTomorrow': 'RainToday'
                        })

"""## Data Granularity and Regional Variance

Weather patterns are highly location-dependent — what holds true for one area in Australia may not apply to another. For example, coastal cities might have very different rainfall behavior compared to inland areas.

Including all locations in the model increases complexity, as it forces the model to generalize across diverse local climates. Instead, narrowing the scope to specific regions may improve model performance and interpretability.

Let’s first explore how many observations we have per location and consider whether it's reasonable to focus on a smaller subset of regions.

## Location Selection and Grouping

To simplify our model and improve accuracy, we aim to focus on a localized region rather than using data from all across Australia.

After researching geographic distances, it was found that:
- **Watsonia** is only ~15 km from **Melbourne**
- **Melbourne Airport** is about ~18 km from **Melbourne**

Given their close proximity, it's reasonable to assume these locations share similar weather patterns. Therefore, we’ll group **Melbourne**, **Watsonia**, and **Melbourne Airport** together and use their combined data to train our localized rain prediction model.

To preserve any subtle differences between them, we’ll still keep the **Location** column as a categorical feature during modeling.
"""

df = df[df.Location.isin(['Melbourne','MelbourneAirport','Watsonia',])]
df. info()

"""We still have 7557 records, which should be enough to build a reasonably good model.  
You could always gather more data if needed by partioning the data into similar locations or simplyby updating it from the source to include a larger time frame.

## Extracting a seasonality feature
Now consider the `Date` column. We expect the weather patterns to be seasonal, having different predictablitiy levels in winter and summer for example.  
There may be some variation with `Year` as well, but we'll leave that out for now.
Let's engineer a `Season` feature from `Date` and drop `Date` afterward, since it is most likely less informative than season.
An easy way to do this is to define a function that assigns seasons to given months, then use that function to transform the `Date` column.

### Create a function to map dates to seasons
"""

def date_to_season(date):
    month = date.month
    if (month == 12) or (month == 1) or (month == 2):
        return 'Summer'
    elif (month == 3) or (month == 4) or (month == 5):
        return 'Autumn'
    elif (month == 6) or (month == 7) or (month == 8):
        return 'Winter'
    elif (month == 9) or (month == 10) or (month == 11):
        return 'Spring'

df['Date'] = pd.to_datetime(df['Date'])
df['Season'] = df['Date'].apply(date_to_season)

df = df.drop(columns = 'Date')
df

"""## Feature Readiness & Target Balance Check

We now have a solid set of features to work with — everything is cleaned, encoded, and ready for modeling.

But before jumping into training, it's important to **check the balance of our target variable**: `RainTomorrow`.  
If the classes are imbalanced (e.g., many more "No" than "Yes" labels), our model might perform poorly on the minority class.

Let’s take a quick look at the class distribution.

"""

X = df.drop(columns='RainToday', axis = 1)
y = df['RainToday']

y.value_counts()

"""## Target Distribution Insights

After examining the `RainTomorrow` target within the Melbourne area (Melbourne, Watsonia, and Melbourne Airport), we found this distribution for **`RainToday`**, which serves as a strong proxy for understanding class imbalance.

This means it **doesn't rain on approximately 76% of days** and it rains on about **24%**.  
If we were to **always predict “No”**, we'd achieve **76% accuracy** by doing absolutely nothing.  
While that might sound good, it's actually misleading, because we’d **completely miss the rainy days**, which are arguably the most important to detect.

In other words, **this is a clearly imbalanced dataset**, and that imbalance can bias our model toward always predicting "No."

---

## Next Steps

To build a meaningful and fair model, we’ll:
- Proceed with feature engineering and train-test split
- Consider techniques to address imbalance (e.g., class weights or resampling)
- Evaluate model performance using **precision**, **recall**, and **F1-score**, not just accuracy

These steps will help us ensure the model performs well not just overall, but especially when predicting rainy days — the key value of this project.

## Train-Test Split

Now, we’ll split the dataset into training and testing sets to evaluate our model’s performance on unseen data.

We use `train_test_split` with the following parameters:

- `test_size=0.2`: Reserves 20% of the data for testing, which is a common split ratio balancing training data sufficiency and reliable evaluation.
- `stratify=y`: Ensures the split maintains the same proportion of rain/no-rain classes in both training and testing sets, preserving class distribution and preventing imbalance issues.
- `random_state=42`: Sets a fixed seed for reproducibility, so the split remains consistent every time we run the notebook.

This approach helps us train the model on representative data while evaluating it fairly on unseen samples.
"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,
                                                    stratify=y, random_state=42)

"""### Defining Preprocessing Transformers for Numerical and Categorical Features

To prepare our data for modeling, we need to preprocess numerical and categorical features differently. Numerical features typically require scaling (e.g., standardization), while categorical features need encoding (e.g., one-hot encoding) to convert them into a format the model can understand.

A good practice is to **automatically detect** which columns are numerical and which are categorical based on their data types. This way, the preprocessing pipeline adapts flexibly to the dataset without hardcoding column names.

Once detected, we assign numerical columns to a numeric transformer (e.g., `StandardScaler`) and categorical columns to a categorical transformer (e.g., `OneHotEncoder`). These transformers will then be combined in a `ColumnTransformer` to apply the correct preprocessing steps to each feature type.


"""

numeric_features = X_train.select_dtypes(include=['number']).columns.tolist()
categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()

""" We define separate transformers for both feature types and combine them into a single preprocessing transformer

"""

# Scale the numeric features
numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])

# One-hot encode the categoricals
categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])

""" We combine the transformers into a single preprocessing column transformer


"""

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ]
)

""" We create a pipeline by combining the preprocessing with a Random Forest classifier

"""

pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier(random_state=42))
])

"""We define a parameter grid to use in a cross validation grid search model optimizer

"""

param_grid = {
    'classifier__n_estimators': [50, 100],
    'classifier__max_depth': [None, 10, 20],
    'classifier__min_samples_split': [2, 5]
}

"""## Pipeline Usage in Cross-Validation

The pipeline is applied repeatedly during cross-validation. For each internal training fold, the pipeline is fitted, and predictions are made on the corresponding validation fold.

This process ensures that all preprocessing steps and model training happen within each fold, preventing data leakage and providing a realistic estimate of model performance.

## Grid Search Cross-Validation and Model Fitting

To find the best hyperparameters for our model, we perform a **grid search** combined with cross-validation. This systematically tests different parameter combinations and evaluates model performance to select the optimal setup.

We use a cross-validation method that **stratifies the target variable** during validation folds. Stratification ensures each fold maintains the same proportion of rain/no-rain instances as the full dataset, which is crucial for reliable evaluation on imbalanced data.

After the grid search identifies the best parameters, we fit the model with those settings on the entire training set to prepare for final evaluation.
"""

cv = StratifiedKFold(n_splits=5, shuffle=True)

"""We instantiate and fit GridSearchCV to the pipeline

"""

grid_search = GridSearchCV(estimator = pipeline, param_grid = param_grid, cv= cv, scoring='accuracy', verbose=2)
grid_search.fit(X_train, y_train)

"""We print the best parameters and best crossvalidation score

"""

print("\nBest parameters found: ", grid_search.best_params_)
print("Best cross-validation score: {:.2f}".format(grid_search.best_score_))

"""Then, we display your model's estimated score

"""

test_score = grid_search.score(X_test, y_test)
print("Test set score: {:.2f}".format(test_score))

"""So we have a reasonably accurate classifer, which is expected to correctly predict about 84% of the time whether it will rain today in the Melbourne area.  
But careful here. Let's take a deeper look at the results.

The best model is stored within the gridsearch object.

Now, we get the model predictions from the grid search estimator on the unseen data
"""

y_pred = grid_search.predict(X_test)

"""And we print the classification report


"""

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

"""## Confusion Matrix Visualization

A confusion matrix provides a detailed summary of the classification results by showing how many predictions were correct and where the model made errors. It breaks down predictions into:

- **True Positives (TP):** Correctly predicted rain days  
- **True Negatives (TN):** Correctly predicted no-rain days  
- **False Positives (FP):** Incorrectly predicted rain when it didn't rain  
- **False Negatives (FN):** Missed rain days (predicted no rain, but it rained)  

This visualization is essential to understand the types of mistakes the model makes, which is especially important in imbalanced datasets like ours where accuracy alone can be misleading.

"""

conf_matrix = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)
disp.plot(cmap='Blues')
plt.title('Confusion Matrix')
plt.show()

"""### Confusion Matrix Analysis

Given the confusion matrix values:  
- True Negatives (TN) = 1084  
- False Positives (FP) = 70  
- False Negatives (FN) = 171  
- True Positives (TP) = 187  

**Is this a good predictor of rainfall?**

- The model correctly predicts rain on **187 days** (TP) but misses rain on **171 days** (FN).  
- It accurately identifies no-rain days on **1084 occasions** (TN), with **70 false alarms** (FP).  

This means the model correctly identifies about **52%** of rainy days. While better than random guessing, it still misses nearly half the rain events, which could be problematic depending on the application’s tolerance for missed rain predictions.

### Feature Importances

To understand which features influence our model’s predictions most, we look at feature importances. For numerical features, this is straightforward since their names remain unchanged throughout preprocessing.

However, for categorical features, it’s a bit more complex. Because we applied one-hot encoding, each original categorical feature was expanded into multiple binary features. To interpret importances meaningfully, we need to map these one-hot encoded features back to their original categorical variables.

Recall that our pipeline used a column transformer named `'cat'` to convert categorical features into one-hot encoded columns. By working backwards through this transformer, we can aggregate the importances of the one-hot encoded features and assign them to their original categorical features.

Next, we’ll collect all feature importances and link them to their corresponding transformed features to get a clear picture of which variables matter most in our model.
"""

feature_importances = grid_search.best_estimator_['classifier'].feature_importances_

"""Now let's extract the feature importances and plot them as a bar graph.

"""

# Combine numeric and categorical feature names
feature_names = numeric_features + list(grid_search.best_estimator_['preprocessor']
                                        .named_transformers_['cat']
                                        .named_steps['onehot']
                                        .get_feature_names_out(categorical_features))

feature_importances = grid_search.best_estimator_['classifier'].feature_importances_

importance_df = pd.DataFrame({'Feature': feature_names,
                              'Importance': feature_importances
                             }).sort_values(by='Importance', ascending=False)

N = 20  # Change this number to display more or fewer features
top_features = importance_df.head(N)

# Plotting
plt.figure(figsize=(10, 6))
plt.barh(top_features['Feature'], top_features['Importance'], color='skyblue')
plt.gca().invert_yaxis()  # Invert y-axis to show the most important feature on top
plt.title(f'Top {N} Most Important Features in predicting whether it will rain today')
plt.xlabel('Importance Score')
plt.show()

"""As can be seen from the graph, `Humidity3pm`, `Pressure9am`, `Sunshine` features have the highest importance scores.

### Trying Another Model

Now, we’ll explore training a different model to see if we can improve performance.

In real projects, it’s common to try various algorithms and revisit your data preprocessing to boost results. You might engineer new features, remove redundant ones, handle missing data more effectively, or reduce dimensionality. You can also expand your hyperparameter search or even incorporate techniques like clustering to create new features and include their parameters in the search.

Thanks to Scikit-learn’s flexible `Pipeline` and `GridSearchCV`, trying out different models and parameter grids is straightforward.

Next, we’ll update our pipeline and parameter grid to train a **Logistic Regression** model, then compare its performance to our previous model. We’ll replace the classifier in the pipeline accordingly and use the provided parameter grid.
"""

# Replace RandomForestClassifier with LogisticRegression
pipeline.set_params(classifier=LogisticRegression(random_state=42))

# update the model's estimator to use the new pipeline
grid_search.estimator = pipeline

# Define a new grid with Logistic Regression parameters
param_grid = {
    'classifier__solver' : ['liblinear'],
    'classifier__penalty': ['l1', 'l2'],
    'classifier__class_weight' : [None, 'balanced']
}

grid_search.param_grid = param_grid

# Fit the updated pipeline with LogisticRegression
grid_search.fit(X_train, y_train)

# Make predictions
y_pred = grid_search.predict(X_test)

"""Now, we will compare the results to your previous model.
Let's display the clasification report and the confusion matrix for the new model and compare your results with the previous model.

"""

print(classification_report(y_test, y_pred))

# Generate the confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

plt.figure()
sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='d')

# Set the title and labels
plt.title('Titanic Classification Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')

# Show the plot
plt.tight_layout()
plt.show()

"""### Comparing Logistic Regression and Random Forest Models

We evaluated two models for rainfall prediction and compared their performance metrics:

| Metric              | Logistic Regression | Random Forest       |
|---------------------|---------------------|---------------------|
| **Accuracy**        | 84%                 | 83%                 |
| **Precision (No)**  | 0.86                | 0.86                |
| **Recall (No)**     | 0.94                | 0.93                |
| **Precision (Yes)** | 0.73                | 0.69                |
| **Recall (Yes)**    | 0.52                | 0.51                |
| **F1-Score (Yes)**  | 0.61                | 0.59                |
| **Support (Yes)**   | 358                 | 358                 |

- Both models have similar overall accuracy (~83-84%) and perform well at predicting **no rain** days with high precision and recall.
- The Logistic Regression model has a slightly better recall for rain days (52% vs 51%), meaning it detects a few more rainy days than Random Forest.
- Precision for rain days is also slightly higher for Logistic Regression, suggesting fewer false alarms compared to Random Forest.
- F1-scores for rain prediction are close, with Logistic Regression marginally ahead.
  
**Conclusion:**  
Both models handle the majority class (no rain) very well but struggle with predicting rain days, reflecting the dataset’s imbalance and the challenging nature of the problem. Logistic Regression edges out Random Forest in detecting rainy days slightly better, but the difference is minor.

To improve both models, focusing on increasing recall (true positive rate) for rain days is crucial. This can be done by advanced feature engineering, handling class imbalance better, or exploring more powerful models like gradient boosting.

### Model Improvement Experiments

Now let's try some techniques to improve the rain detection of the logistic regression model, since our models
only catch about 52% of rainy days. We'll test:

1. Class weight adjustments
2. Optimizing for recall instead of accuracy  
3. Custom decision thresholds
4. Simple feature engineering

Then, we will compare the current version with the final version to see the improvements of the changes.
"""

# Create a new pipeline with class weight focus
pipeline_improved = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', LogisticRegression(random_state=42))
])

# Expanded parameter grid with class weights
param_grid_improved = {
    'classifier__solver': ['liblinear'],
    'classifier__penalty': ['l1', 'l2'],
    'classifier__class_weight': [
        None,           # No adjustment (baseline)
        'balanced',     # Automatically balance based on class frequency

    ]
}

# Grid search optimizing for recall instead of accuracy
grid_search_improved = GridSearchCV(
    estimator=pipeline_improved,
    param_grid=param_grid_improved,
    cv=cv,
    verbose=1,
    n_jobs=-1  # Use all CPU cores for faster training
)

# Fit the improved model
grid_search_improved.fit(X_train, y_train)

y_pred_improved = grid_search_improved.predict(X_test)
print(classification_report(y_test, y_pred_improved))

"""This alone didn't change a lot. Let's continue with defining a custom threshold."""

# Use the improved model from step 1 to get prediction probabilities
y_proba = grid_search_improved.predict_proba(X_test)[:, 1]  # Probability of 'Yes' (rain)

# Test different thresholds
thresholds = [0.3, 0.35, 0.4, 0.45, 0.5]
threshold_results = []

def calculate_metrics(y_true, y_pred):
    return {
        'Precision (Rain)': precision_score(y_true, y_pred, pos_label='Yes'),
        'Recall (Rain)': recall_score(y_true, y_pred, pos_label='Yes'),
        'F1-Score (Rain)': f1_score(y_true, y_pred, pos_label='Yes'),
        'Accuracy': accuracy_score(y_true, y_pred)
    }

for threshold in thresholds:
    # Apply custom threshold
    y_pred_threshold = ['Yes' if prob >= threshold else 'No' for prob in y_proba]

    # Calculate metrics
    metrics = calculate_metrics(y_test, y_pred_threshold)
    metrics['Threshold'] = threshold
    threshold_results.append(metrics)

    print(f"Threshold {threshold}: Recall={metrics['Recall (Rain)']:.3f}, "
          f"Precision={metrics['Precision (Rain)']:.3f}, "
          f"Accuracy={metrics['Accuracy']:.3f}")

# Find best threshold based on F1-score
best_threshold_idx = max(range(len(threshold_results)),
                        key=lambda i: threshold_results[i]['F1-Score (Rain)'])
best_threshold = thresholds[best_threshold_idx]
best_threshold_metrics = threshold_results[best_threshold_idx]

print(f"\n🎯 Best threshold: {best_threshold} (F1-Score: {best_threshold_metrics['F1-Score (Rain)']:.3f})")

y_pred_best_threshold = ['Yes' if prob >= best_threshold else 'No' for prob in y_proba]

print("\nClassification Report with Best Threshold:")
print(classification_report(y_test, y_pred_best_threshold))

"""As we can see, the recall improved. Let's continue with our next steps. To improve the recall score, we will compute new features using the available data.
### Why Create New Features?

Raw weather data often misses hidden patterns. To help the model better detect signs of rainfall, we created new features that show **how conditions change during the day**:

- `PressureDrop`: Falling pressure often signals incoming rain.
- `TempRange`: Captures daily temperature variation.
- `HumidityIncrease`: Higher afternoon humidity can mean more moisture in the air.
- `WindSpeedIncrease` and `CloudIncrease`: Shifting winds and growing clouds often appear before rain.

These engineered features highlight weather dynamics and should improve the model's ability to catch true rain events more accurately.

We create copies of the original train and test sets
"""

X_train_enhanced = X_train.copy()
X_test_enhanced = X_test.copy()

"""Then, we define the `add_weather_features` function to calculate the new features, and add them to the sets"""

def add_weather_features(df):
    """Add simple weather-related features"""
    df_enhanced = df.copy()

    # Pressure changes (indicator of weather fronts)
    df_enhanced['PressureDrop'] = df_enhanced['Pressure9am'] - df_enhanced['Pressure3pm']

    # Temperature variation (daily temperature range)
    df_enhanced['TempRange'] = df_enhanced['MaxTemp'] - df_enhanced['MinTemp']

    # Humidity changes throughout the day
    df_enhanced['HumidityIncrease'] = df_enhanced['Humidity3pm'] - df_enhanced['Humidity9am']

    # Wind speed changes
    if 'WindSpeed9am' in df_enhanced.columns and 'WindSpeed3pm' in df_enhanced.columns:
        df_enhanced['WindSpeedIncrease'] = df_enhanced['WindSpeed3pm'] - df_enhanced['WindSpeed9am']

    # Temperature at specific times vs daily extremes
    if 'Temp9am' in df_enhanced.columns:
        df_enhanced['Temp9am_vs_Min'] = df_enhanced['Temp9am'] - df_enhanced['MinTemp']
    if 'Temp3pm' in df_enhanced.columns:
        df_enhanced['Temp3pm_vs_Max'] = df_enhanced['MaxTemp'] - df_enhanced['Temp3pm']

    # Cloud cover changes
    if 'Cloud9am' in df_enhanced.columns and 'Cloud3pm' in df_enhanced.columns:
        df_enhanced['CloudIncrease'] = df_enhanced['Cloud3pm'] - df_enhanced['Cloud9am']

    return df_enhanced

"""We update the train and test sets"""

X_train_enhanced = add_weather_features(X_train_enhanced)
X_test_enhanced = add_weather_features(X_test_enhanced)

"""Lets take a look at the number of features"""

print(f"Original features: {len(X_train.columns)}")
print(f"Enhanced features: {len(X_train_enhanced.columns)}")
print(f"New features added: {len(X_train_enhanced.columns) - len(X_train.columns)}")

# Show the new features
new_features = [col for col in X_train_enhanced.columns if col not in X_train.columns]
print(f"New features: {new_features}")

# Update the preprocessor for new numeric features
numeric_features_enhanced = X_train_enhanced.select_dtypes(include=['number']).columns.tolist()
categorical_features_enhanced = X_train_enhanced.select_dtypes(include=['object', 'category']).columns.tolist()

# New preprocessor with enhanced features
preprocessor_enhanced = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numeric_features_enhanced),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features_enhanced)
    ]
)

# Enhanced pipeline
pipeline_enhanced = Pipeline(steps=[
    ('preprocessor', preprocessor_enhanced),
    ('classifier', LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000))
])
# Train the improved model with enhanced features
pipeline_enhanced.fit(X_train_enhanced, y_train)

# Make predictions
y_pred_enhanced = pipeline_enhanced.predict(X_test_enhanced)
print("\n Enhanced Model Classification Report:")
print(classification_report(y_test, y_pred_enhanced))

"""As we can see, recall has improved significantly to 0.74. However, this comes with a trade-off, as accuracy has dropped slightly to 0.77. Let's now examine whether adding the custom threshold method would increase this enhanced model's recall."""

y_proba_custom = pipeline_enhanced.predict_proba(X_test_enhanced)[:, 1]  # Probability of 'Yes' (rain)
threshold_enhanced = 0.45
y_pred_enhanced_custom = ['Yes' if prob >= threshold_enhanced else 'No' for prob in y_proba_custom]

print("\nClassification Report with Enhanced Model and Custom Threshold:")
print(classification_report(y_test, y_pred_enhanced_custom))

"""As we can see, the recall increased once more to 0.78, which means the model can correctly predict rainy days almost 80% of the time. Let's compare this to the original model."""

# Calculate metrics for all models
models_results = {
    'Original (Accuracy Optimized)': calculate_metrics(y_test, y_pred),
    'Class Balanced': calculate_metrics(y_test, y_pred_improved),
    'Best Threshold': calculate_metrics(y_test, y_pred_best_threshold),
    'Feature Enhanced': calculate_metrics(y_test, y_pred_enhanced),
    'Feature Enhanced with Custom Threshold': calculate_metrics(y_test, y_pred_enhanced_custom)
}

# Create comprehensive comparison DataFrame
comparison_final = pd.DataFrame(models_results).T

# Round for better display
comparison_final = comparison_final.round(3)

print("COMPREHENSIVE MODEL COMPARISON")
print("=" * 80)
print(comparison_final)

# Create visualization comparing all models
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))

# 1. Recall comparison
models = list(models_results.keys())
recalls = [models_results[model]['Recall (Rain)'] for model in models]
colors = ['lightblue', 'lightgreen', 'orange', 'lightcoral', 'lightpink']

ax1.bar(models, recalls, color=colors)
ax1.set_title('Rain Detection Recall by Model')
ax1.set_ylabel('Recall (Rain)')
ax1.set_ylim(0, 1)
plt.setp(ax1.get_xticklabels(), rotation=45, ha='right')
for i, v in enumerate(recalls):
    ax1.text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')

# 2. Precision vs Recall scatter
precisions = [models_results[model]['Precision (Rain)'] for model in models]
ax2.scatter(recalls, precisions, c=colors, s=100, alpha=0.7)
for i, model in enumerate(models):
    ax2.annotate(model, (recalls[i], precisions[i]), xytext=(5, 5),
                textcoords='offset points', fontsize=8)
ax2.set_xlabel('Recall (Rain)')
ax2.set_ylabel('Precision (Rain)')
ax2.set_title('Precision vs Recall Trade-off')
ax2.grid(True, alpha=0.3)

# 3. F1-Score comparison
f1_scores = [models_results[model]['F1-Score (Rain)'] for model in models]
ax3.bar(models, f1_scores, color=colors)
ax3.set_title('F1-Score (Rain) by Model')
ax3.set_ylabel('F1-Score (Rain)')
ax3.set_ylim(0, 1)
plt.setp(ax3.get_xticklabels(), rotation=45, ha='right')
for i, v in enumerate(f1_scores):
    ax3.text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')

# 4. Overall accuracy
accuracies = [models_results[model]['Accuracy'] for model in models]
ax4.bar(models, accuracies, color=colors)
ax4.set_title('Overall Accuracy by Model')
ax4.set_ylabel('Accuracy')
ax4.set_ylim(0.7, 0.9)  # Zoom in on relevant range
plt.setp(ax4.get_xticklabels(), rotation=45, ha='right')
for i, v in enumerate(accuracies):
    ax4.text(i, v + 0.001, f'{v:.3f}', ha='center', va='bottom')

plt.tight_layout()
plt.show()

# Find the best model based on different criteria
best_recall = max(models_results.keys(), key=lambda x: models_results[x]['Recall (Rain)'])
best_precision = max(models_results.keys(), key=lambda x: models_results[x]['Precision (Rain)'])
best_f1 = max(models_results.keys(), key=lambda x: models_results[x]['F1-Score (Rain)'])
best_accuracy = max(models_results.keys(), key=lambda x: models_results[x]['Accuracy'])

print("\n" + "="*60)
print("BEST MODEL BY DIFFERENT CRITERIA")
print("="*60)
print(f"🎯 Best Recall (Rain Detection): {best_recall}")
print(f"   - Catches {models_results[best_recall]['Recall (Rain)']:.1%} of rainy days")

print(f"\n🔍 Best Precision (Fewer False Alarms): {best_precision}")
print(f"   - {models_results[best_precision]['Precision (Rain)']:.1%} of rain predictions are correct")

print(f"\n⚖️ Best F1-Score (Balance): {best_f1}")
print(f"   - F1-Score: {models_results[best_f1]['F1-Score (Rain)']:.3f}")

print(f"\n📊 Best Overall Accuracy: {best_accuracy}")
print(f"   - Overall accuracy: {models_results[best_accuracy]['Accuracy']:.1%}")

# Summary and recommendations
print("\n" + "="*60)
print("SUMMARY & RECOMMENDATIONS")
print("="*60)

original_recall = models_results['Original (Accuracy Optimized)']['Recall (Rain)']
best_recall_value = models_results[best_recall]['Recall (Rain)']
improvement = best_recall_value - original_recall

print(f"✅ Best improvement in rain detection: +{improvement:.1%}")
print(f"✅ Now catching {best_recall_value:.1%} of rainy days vs {original_recall:.1%} originally")

print(f"\n🎯 For weather prediction applications:")
print(f"   - Use '{best_recall}' if missing rain is costly")
print(f"   - Use '{best_precision}' if false alarms are problematic")
print(f"   - Use '{best_f1}' for best overall balance")